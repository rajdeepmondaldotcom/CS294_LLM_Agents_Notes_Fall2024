# Lecture 10

This [lecture](https://www.youtube.com/live/f3KKx9LWntQ?si=18thoCOqkb9nR6D_) goes in depth on how access to foundation models profoundly shapes the types of **agents** and **research** we can pursue. With only **API access**, we can build multi-step problem-solvers or social simulators, but the models remain static and hidden. **Open-weight models** enable reproducibility, interpretability, and partial customization yet still remain confined to the original architecture and data. True **open-source** frameworks unlock full control (data, training code, compute) for deeper experimentation and fundamental breakthroughs—but demand significant infrastructure. Ultimately, developing and refining AI requires rethinking **all layers**—from data mixtures to model architecture to training recipes—in truly open ecosystems to drive the next wave of innovation.

---

## Introduction

| **Topic**  | **Notes**                                                                                                                                                                                                                                                 |
| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Rise of Large AI Models**      | - Models’ **capabilities** skyrocketed recently (GPT-4, Claude, etc.). <br>- However, **openness** (weights, code, data) has *plummeted* → mostly “API-only” access.                                                                                               |
| **Why Access Matters**           | - **Access shapes research**: <br>   - 1990s: Statistical NLP took off thanks to large internet text corpora. <br>   - 2010s: Crowd-sourced data sets (ImageNet, SQuAD) spurred deep learning progress. <br>   - GPUs (hardware access) enabled new breakthroughs. |
| **Three Levels of Model Access** | 1. **API-only** (black box): We can only prompt it. <br>2. **Open-Weight**: We can see and manipulate the trained parameters. <br>3. **Fully Open-Source**: We control data, architecture, training code, etc.                                                     |
| **Goal of Lecture**              | - Explore how these access tiers influence what kind of **agents** and **research** are possible.                                                                                                                                                                  |

---

## Black-Box API Agents

| **Topic**                     | **Notes**                                                                                                                                                                                                                                                                                           |
|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Agent Architecture**                               | - Agent maintains a **memory stream** of observations. <br>- On each step: retrieve relevant memories → produce actions (tool usage, planning, reflection). <br>- All powered by **language model API** calls.                                                                                                                                       |
| **Problem-Solving Agents**                           | 1. **MLAgentBench**: <br>   - Simulates a data science/kaggle scenario: agent has code, data, eval routine → tries to produce best ML model. <br>   - **Prompt** includes reflection, planning, fact-checking → agent loops code edits & sees logs. <br>   - Performance: top models do well on some tasks, fail on more complex tasks (e.g., BabyLM).  <br>2. **Cybersecurity**: <br>   - “Capture the flag” challenges: agent sees server code, interacts via bash to find vulnerabilities. <br>   - Current best model solves ~17.5% of tasks; humans do far better.                                                                                               |
| **Simulation Agents**                                | - *Smallville* environment: 25 generative agents, each LM-based. <br>- Agents store mundane event logs but do **retrieval** (recency, importance, relevance) to limit prompt size. <br>- They also do “reflection”: aggregating events into higher-level beliefs/personality traits. <br>- *Social info diffusion:* Agents share news (e.g., party) among each other. |
| **Real People Simulation**                           | - New approach: *interview* 1,000 real humans (2-hour transcripts) → seed each agent with that personal background. <br>- Then compare agent’s behavior with the real person’s behavior on surveys, psychological tests, etc. <br>- Agents match ~69% of the human’s own responses (where the human’s retest reliability is ~81%).                                                        |
| **Reflections on Black-Box**                        | - **Powerful** for building dynamic, flexible multi-step agents. <br>- But all are “static” in that the LLM can’t *update weights* from new experiences. <br>- Next step: we want RL or iterative fine-tuning → need more than just API access.                                                                                                     |

---

## Open-Weight Models

| **Topic**               | **Notes**                                                                                                                                                                                                                                                        |
|----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Advantages of Open Weights**               | 1. **Reproducibility**: No risk of model “disappearing” or silent updates. <br>2. **Interpretability & Debugging**: Inspect neurons, see triggers for strange outputs. <br>3. **Customization**: Distillation, merging, pruning.                                                                                 |
| **Interpretability Example**                 | - *TranslucE* work: Neuron-level visualization for Llama → discovered a neuron that fires for “9/11,” causing confusion about “9.11” vs. “9.8.”                                                                                                                           |
| **Distillation / Pruning**                   | - NVIDIA approach: prunes layers from a 15B model to 8B or 4B with minimal performance drop → *healing* the model afterwards with small fine-tuning.                                                                                                                      |
| **Adversarial Attacks**                      | - Jailbreak prompts *optimized* on open-weight Llama → *transfer* to closed models (GPT-4). <br>- Indicates open-model attacks can break closed-source ones.                                                                                                                                                     |
| **Model Independence Testing** (Sally et al.) | - We want to see if Model B was trained independently from Model A or *fine-tuned* from it. <br>- *Permutation-based p-value test:* If B shares highly correlated parameters with A, it’s “not independent.” <br>- Found real examples (e.g., Mistral leak) by comparing suspicious models’ layers.                                                         |
| **Reflections on Open Weights**              | - **Huge boon** to research: interpretability, novel tasks, and new technical problems (like independence testing). <br>- Still confined to “pre-chosen architecture & data.” <br>- Next step: **Open Source** to control the entire pipeline (training procedure, architecture, data).                                                              |

---

## Fully Open-Source Models

| **Topic**                   | **Notes**                                                                                                                                                                                                                                                                  |
|--------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Definition**                                    | - **Open Source** in the software sense: (1) Freely usable, (2) Studied, (3) Modified, (4) Redistributed. <br>- For AI: Must have **weights, training code, data processing pipeline**. <br>- But data is often web-scraped with ambiguous licenses, so typically only *metadata* is shared to avoid legal issues. |
| **Importance of Compute**                         | - Full retraining or big changes require **massive GPU resources**. <br>- *Challenges:* Large scales are dominated by closed labs. <br>- *Possible solutions:* (1) **Scaling laws** extrapolation at small scale, (2) **Idle GPU pooling** (decentralized approach), (3) **Publicly funded compute** (NAIRR).                                           |
| **Research Freedoms**                             | - With **full pipeline** control, we can question data mixture, training recipe, architecture → leading to deeper breakthroughs. <br>- Without open source, many crucial research directions remain blocked.                                                                                                                |
| **Examples of Projects**                          | 1. **Data Mixture Optimization**: Automatic weighting of code/Wikipedia/book corpora → 2.6× training speedup. <br>2. **Better Optimizers**: e.g., second-order info → 2× faster training than Adam. <br>3. **Alternate Architectures**: e.g., “Backpack LLM” with interpretable hidden units.                                         |

---